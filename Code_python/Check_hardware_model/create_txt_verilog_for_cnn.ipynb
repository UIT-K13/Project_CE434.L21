{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd05c3f60802471494b60a2e66fd4ecd6fdf92055af140e68ee192931e29e235180",
   "display_name": "Python 3.8.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bitstring import Bits\n",
    "import pickle\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quantization2d(data_arr, min_q, max_q, scale, offset):\n",
    "\trow, col = data_arr.shape\n",
    "\tdata_quantization = np.ones((row, col))\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\tdata_quantization[i][j] = round((data_arr[i][j] / scale) + offset)\n",
    "\n",
    "\t\t\tif(data_quantization[i][j] > max_q):\n",
    "\t\t\t\tdata_quantization[i][j] = max_q\n",
    "\t\t\telif(data_quantization[i][j] < min_q):\n",
    "\t\t\t\tdata_quantization[i][j] = min_q\n",
    "\n",
    "\treturn data_quantization\n",
    "\n",
    "def Quantization3d(data, min_q, max_q, scale, offset):\n",
    "\trow, col, num_filter = data.shape\n",
    "\tdata_quantization = np.ones((row, col, num_filter))\n",
    "\n",
    "\tfor i in range(num_filter):\n",
    "\t\tdata_quantization[:, :, i] = Quantization2d(data[:, :, i], min_q, max_q, scale, offset)\n",
    "        \n",
    "\treturn data_quantization\n",
    "\n",
    "def Quantization4d(data, min_q, max_q, scale, offset):\n",
    "\trow, col, num_subfilter, num_filter = data.shape\n",
    "\tdata_quantization = np.ones((row, col, num_subfilter, num_filter))\n",
    "\n",
    "\tfor i in range(num_filter):\n",
    "\t\tdata_quantization[:, :, :, i] = Quantization3d(data[:, :, :, i], min_q, max_q, scale, offset)\n",
    "\n",
    "\treturn data_quantization\n",
    "\n",
    "def Dequantization2d(data_arr, scale, offset):\n",
    "\trow, col = data_arr.shape\n",
    "\tdata_dequantization = np.ones((row, col))\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\tdata_dequantization[i][j] = (data_arr[i][j] - offset) * scale\n",
    "\n",
    "\treturn data_dequantization\n",
    "\n",
    "def\tRound_quantization2d(data_arr, min_q, max_q):\n",
    "\trow, col = data_arr.shape\n",
    "\tdata_round_quantization = np.ones((row, col))\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\tdata = float(data_arr[i][j])\n",
    "\t\t\tdata_round_quantization[i][j] = round(data)\n",
    "\n",
    "\t\t\tif(data_round_quantization[i][j] > max_q):\n",
    "\t\t\t\tdata_round_quantization[i][j] = max_q\n",
    "\t\t\telif(data_round_quantization[i][j] < min_q):\n",
    "\t\t\t\tdata_round_quantization[i][j] = min_q\n",
    "\n",
    "\treturn data_round_quantization\n",
    "\n",
    "def\tRound_quantization3d(data, min_q, max_q):\n",
    "\trow, col, num_filter = data.shape\n",
    "\tdata_round_quantization = np.ones((row, col, num_filter))\n",
    "\n",
    "\tfor i in range(num_filter):\n",
    "\t\tdata_round_quantization[:, :, i] = Round_quantization2d(data[:, :, i], min_q, max_q)\n",
    "\n",
    "\treturn data_round_quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(data):\n",
    "\tif(data >= 0):\n",
    "\t\treturn data\n",
    "\telse: \n",
    "\t\treturn 0\n",
    "def ReLU_2d(data):\n",
    "\trow, col = data.shape\n",
    "\tdata_ret = np.zeros((row, col))\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\tdata_ret[i, j] = ReLU(data[i, j])\n",
    "\t\n",
    "\treturn data_ret\n",
    "\n",
    "def ReLU_3d(data):\n",
    "\trow, col, num_filter = data.shape\n",
    "\tdata_ret = np.zeros((row, col, num_filter))\n",
    "\n",
    "\tfor i in range(num_filter):\n",
    "\t\tdata_ret[:, :, i] = ReLU_2d(data[:, :, i])\n",
    "\n",
    "\treturn data_ret\n",
    "\n",
    "def Sigmoid(data):\n",
    "    if(data < -4):\n",
    "        return 0\n",
    "    elif((-4 <= data) and (data < 0)):\n",
    "        return (1/2)*((1+(data/4))**2)\n",
    "    elif((0 <= data) and (data < 4)):\n",
    "        return 1-(1/2)*((1-(data/4))**2)\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def Sigmoid_2d(data):\n",
    "\trow, col = data.shape\n",
    "\tdata_ret = np.zeros((row, col))\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\tdata_ret[i, j] = Sigmoid(data[i, j])\n",
    "\t\n",
    "\treturn data_ret\n",
    "\n",
    "def max_pooling(matrix):\n",
    "\trow, col = matrix.shape\n",
    "\n",
    "\tmax_value = 0\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\tif(matrix[i][j] > max_value):\n",
    "\t\t\t\tmax_value = matrix[i][j]\n",
    "\treturn max_value\n",
    "\n",
    "def avg_pooling(matrix):\n",
    "\trow, col = matrix.shape\n",
    "\n",
    "\tsum_value = 0\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\t\tsum_value = sum_value + matrix[i][j]\n",
    "\n",
    "\tavg_value = sum_value/(col*row)\n",
    "\treturn avg_value\n",
    "\n",
    "def median_pooling(matrix):\n",
    "\trow, col = matrix.shape\n",
    "\n",
    "\tmatrix = np.reshape(matrix, (1, row*col))\n",
    "\n",
    "\tmatrix = np.sort(matrix)\n",
    "\n",
    "\tnum_element = row*col\n",
    "\n",
    "\tmedian_index = int(num_element/2)\n",
    "\n",
    "\treturn matrix[0, median_index]\n",
    "\n",
    "def pooling_2d(x_matrix, dim_filter, stride, padding, pooling_option):\n",
    "\tx_row, x_col = x_matrix.shape\n",
    "\tfilter_row, filter_col = dim_filter\n",
    "\n",
    "\ty_row = int(((x_row - filter_row + 2*padding)/stride) + 1)\n",
    "\ty_col = int(((x_col - filter_col + 2*padding)/stride) + 1)\n",
    "\n",
    "\tif(padding >= 1):\n",
    "\t\ttemp_matrix = x_matrix\n",
    "\t\tx_row = x_row + 2*padding\n",
    "\t\tx_col = x_col + 2*padding\n",
    "\t\tx_matrix = np.zeros((x_row, x_col))\n",
    "\t\tx_matrix[0+padding:x_row-padding, 0+padding:x_col-padding] = temp_matrix\n",
    "\n",
    "\ty = np.zeros((y_row, y_col))\n",
    "\n",
    "\tindex_row = np.arange(start=0, stop=x_row-filter_row+1, step=stride)\n",
    "\tindex_col = np.arange(start=0, stop=x_col-filter_col+1, step=stride)\n",
    "\tfor i in range(len(index_row)):\n",
    "\t\tfor j in range(len(index_col)):\n",
    "\t\t\tsub_matrix = x_matrix[index_row[i]:index_row[i]+filter_row, index_col[j]:index_col[j]+filter_col]\n",
    "\n",
    "\t\t\tif(pooling_option == 0):\n",
    "\t\t\t\ty[i, j] = max_pooling(sub_matrix)\n",
    "\t\t\telif(pooling_option == 1):\n",
    "\t\t\t\ty[i, j] = avg_pooling(sub_matrix)\n",
    "\t\t\telse:\n",
    "\t\t\t\ty[i, j] = median_pooling(sub_matrix)\n",
    "\t\t\t\n",
    "\treturn y\n",
    "\n",
    "def pooling_3d(x_matrix, dim_filter, stride, padding, pooling_option):\n",
    "\tx_row, x_col, num_filter = x_matrix.shape\n",
    "\tfilter_row, filter_col = dim_filter\n",
    "\n",
    "\ty_row = int(((x_row - filter_row + 2*padding)/stride) + 1)\n",
    "\ty_col = int(((x_col - filter_col + 2*padding)/stride) + 1)\n",
    "\n",
    "\ty = np.zeros((y_row, y_col, num_filter))\n",
    "\n",
    "\tfor i in range(num_filter):\n",
    "\t\tif(pooling_option == 0):\n",
    "\t\t\ty[:, :, i] = pooling_2d(x_matrix[:, :, i], dim_filter, stride, padding, pooling_option)\n",
    "\t\telif(pooling_option == 1):\n",
    "\t\t\ty[:, :, i] = pooling_2d(x_matrix[:, :, i], dim_filter, stride, padding, pooling_option)\n",
    "\t\telse:\n",
    "\t\t\ty[:, :, i] = pooling_2d(x_matrix[:, :, i], dim_filter, stride, padding, pooling_option)\n",
    "\n",
    "\treturn y\n",
    "\n",
    "def conv(x_matrix, filter_matrix):\n",
    "\trow, col = x_matrix.shape\n",
    "\ty = np.zeros((row, col))\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\ty[i, j] = x_matrix[i, j]*filter_matrix[i, j]\n",
    "\t\n",
    "\treturn y.sum()\n",
    "\t\n",
    "def conv2(x_matrix, filter_matrix, stride, padding):\n",
    "\tx_row, x_col = x_matrix.shape\n",
    "\tfilter_row, filter_col = filter_matrix.shape \n",
    "\n",
    "\ty_row = int(((x_row - filter_row + 2*padding)/stride) + 1)\n",
    "\ty_col = int(((x_col - filter_col + 2*padding)/stride) + 1)\n",
    "\n",
    "\tif(padding >= 1):\n",
    "\t\ttemp_matrix = x_matrix\n",
    "\t\tx_row = x_row + 2*padding\n",
    "\t\tx_col = x_col + 2*padding\n",
    "\t\tx_matrix = np.zeros((x_row, x_col))\n",
    "\t\tx_matrix[0+padding:x_row-padding, 0+padding:x_col-padding] = temp_matrix\n",
    "\n",
    "\ty = np.zeros((y_row, y_col))\n",
    "\n",
    "\tindex_row = np.arange(start=0, stop=x_row-filter_row+1, step=stride)\n",
    "\tindex_col = np.arange(start=0, stop=x_col-filter_col+1, step=stride)\n",
    "\tfor i in range(len(index_row)):\n",
    "\t\tfor j in range(len(index_col)):\n",
    "\t\t\tsub_matrix = x_matrix[index_row[i]:index_row[i]+filter_row, index_col[j]:index_col[j]+filter_col]\n",
    "\t\t\ty[i, j] = conv(sub_matrix, filter_matrix)\n",
    "\treturn y\n",
    "\n",
    "def conv3(x_matrix, filter_matrix, stride, padding):\n",
    "\trow_filter, col_filter, num_sub_filter, num_filter = filter_matrix.shape \t\t\t\t# row_filter_matrix = col_filter_matrix\n",
    "\trow_x, col_x, num_x = x_matrix.shape \t\t\t\t\t\t\t\t\t\t\t\t\t# row_x = col_x\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# num_sub_filter = num_x\n",
    "\ty_row = int(((row_x - row_filter + 2*padding)/stride) + 1)\n",
    "\ty_col = int(((col_x - col_filter + 2*padding)/stride) + 1)\n",
    "\t\n",
    "\tdata = np.zeros((y_row, y_col, num_filter))\n",
    "\tfor i in range(num_filter):\n",
    "\t\tdata_temp = np.zeros((y_row, y_col))\n",
    "\n",
    "\t\tfor j in range(num_sub_filter):\n",
    "\t\t\tdata_temp = data_temp + conv2(x_matrix[:, :, j], filter_matrix[:, :, j, i], stride, padding)\n",
    "\t\t\n",
    "\t\tdata[:, :, i] = data_temp\n",
    "\n",
    "\treturn data\n",
    "\n",
    "def add_bias_after_conv(conv_result, bias):\n",
    "\trow, col, num_filter = conv_result.shape\n",
    "\n",
    "\tfor i in range(num_filter):\n",
    "\t\tconv_result[:, :, i] = conv_result[:, :, i] + bias[i, :]\n",
    "\n",
    "\treturn conv_result\n",
    "\n",
    "def Feed_forward_Lenet_5_quantization(image, conv0_filter, bias_conv0, conv1_filter, bias_conv1, W0, W1, W2, B0, B1, B2, scale_calculate_convolution_0, scale_for_pooling0, scale_calculate_convolution_1, scale_for_pooling1, scale_for_W0_x_Flatten, scale_for_W1_x_hd0, scale_for_W2_x_hd1, scale_output, scale_for_B0_conv, scale_for_B1_conv, scale_for_B0_fc, scale_for_B1_fc, scale_for_B2_fc, max_conv0_q, min_conv0_q, max_conv1_q, min_conv1_q, max_pooling0_q, min_pooling0_q, max_pooling1_q, min_pooling1_q, max_hd0_q, min_hd0_q, max_hd1_q, min_hd1_q, max_bf_sigmoid_q, min_bf_sigmoid_q, scale_bf_sigmoid):\n",
    "\tstride = 1\n",
    "\tpadding = 0\n",
    "\tstride_pooling = 2\n",
    "\tpadding_pooling = 0\n",
    "\tpooling_option = 1\n",
    "\n",
    "\t# Convolution layer 0\n",
    "\tconv0_result = conv3(image, conv0_filter, stride, padding)\n",
    "\tadd_bias_after_conv0_bf_relu = Round_quantization3d(add_bias_after_conv(conv0_result*scale_calculate_convolution_0, bias_conv0*scale_for_B0_conv), min_conv0_q, max_conv0_q)\n",
    "\tadd_bias_after_conv0 = ReLU_3d(add_bias_after_conv0_bf_relu)\n",
    "\tpooling0 = Round_quantization3d(pooling_3d(add_bias_after_conv0, (2, 2), stride_pooling, padding_pooling, pooling_option)*scale_for_pooling0, min_pooling0_q, max_pooling0_q)\n",
    "\n",
    "\t# Convolution layer 1\n",
    "\tconv1_result = conv3(pooling0, conv1_filter, stride, padding)\n",
    "\tadd_bias_after_conv1_bf_relu = Round_quantization3d(add_bias_after_conv(conv1_result*scale_calculate_convolution_1, bias_conv1*scale_for_B1_conv), min_conv1_q, max_conv1_q)\n",
    "\tadd_bias_after_conv1 = ReLU_3d(add_bias_after_conv1_bf_relu)\n",
    "\tpooling1 = Round_quantization3d(pooling_3d(add_bias_after_conv1, (2, 2), stride_pooling, padding_pooling, pooling_option)*scale_for_pooling1, min_pooling1_q, max_pooling1_q)\n",
    "\t\n",
    "\t# Fully connected\n",
    "\tflatten = pooling1.reshape(-1, 1)\n",
    "\tprint('input\\n', flatten)\n",
    "\thd_layer0_bf_relu = Round_quantization2d(np.dot(W0, flatten)*scale_for_W0_x_Flatten + B0*scale_for_B0_fc, min_hd0_q, max_hd0_q)\n",
    "\thd_layer0 = ReLU_2d(hd_layer0_bf_relu)\n",
    "\tprint('hd0\\n', hd_layer0)\n",
    "\thd_layer1_bf_relu = Round_quantization2d(np.dot(W1, hd_layer0)*scale_for_W1_x_hd0 + B1*scale_for_B1_fc, min_hd1_q, max_hd1_q) \n",
    "\thd_layer1 = ReLU_2d(hd_layer1_bf_relu)\n",
    "\tprint('hd1\\n', hd_layer1)\n",
    "\toutput_bf_sigmoid = Round_quantization2d(np.dot(W2, hd_layer1)*scale_for_W2_x_hd1 + B2*scale_for_B2_fc, min_bf_sigmoid_q, max_bf_sigmoid_q)\n",
    "\toutput = Sigmoid_2d(Dequantization2d(output_bf_sigmoid, scale_bf_sigmoid, 0))\n",
    "\tprint('output\\n', output)\n",
    "\n",
    "\treturn np.argmax(output), flatten\n",
    "\n",
    "def Load_Conv_and_W_and_Bias(data_path, load_option, conv_shape):\n",
    "\ttemp_data = np.loadtxt(data_path)\n",
    "\n",
    "\tif(load_option == 0): # ================== Conv\n",
    "\t\trow, col, num_sub_filter, num_filter = conv_shape # row = col\n",
    "\t\tdata = np.zeros((row, col, num_sub_filter, num_filter))\n",
    "\n",
    "\t\tindex = 0 # maximum index = num_sub_filter * num_filter\n",
    "\t\tfor m in range(num_filter):\n",
    "\t\t\tfor k in range(num_sub_filter):\n",
    "\t\t\t\tdata[:, :, k, m] = temp_data[index * row: index * row + 5, :]\n",
    "\t\t\t\tindex = index + 1\n",
    "\t\treturn data\n",
    "\telif(load_option == 1): # ================ W\n",
    "\t\treturn temp_data\n",
    "\telse: # ============================= Bias\n",
    "\t\treturn temp_data.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_int_to_binary(number, bit_width):\n",
    "\tif((bit_width%4) != 0):\n",
    "\t\treturn str(Bits(int=number, length=bit_width))\n",
    "\telse:\n",
    "\t\ttemp =  str(Bits(int=number, length=bit_width+1))\n",
    "\t\treturn '0b' + temp[-(len(temp)-3):]\n",
    "\n",
    "def convert_binary_to_hex(number, bit_width):\n",
    "\tif(bit_width <= 4):\n",
    "\t\treturn hex(int(number, 2))\n",
    "\telse:\n",
    "\t\tnumber = number[2:len(number)]\n",
    "\t\tdata_ret = ''\n",
    "\t\tindex = np.arange(bit_width, 1, -4)\n",
    "\t\tfor i in range(len(index)):\n",
    "\t\t\tif(index[i] >= 4):\n",
    "\t\t\t\ttemp = hex(int(number[index[i] - 4 : index[i]], 2))\n",
    "\t\t\t\tdata_ret = temp[-1:] + data_ret\n",
    "\t\t\telse:\n",
    "\t\t\t\ttemp = hex(int(number[0 : index[i]], 2))\n",
    "\t\t\t\tdata_ret = temp[-1:] + data_ret\n",
    "\t\t\n",
    "\t\treturn '0x' + data_ret\n",
    "\n",
    "def create_txt_file_for_w_verilog(data_matrix, num_mutiplier_parallel, bit_width, file_path):\n",
    "\tverilog_file = open(file_path, mode = 'w')\n",
    "\n",
    "\tnum_node_next_layer, num_node_previous_layer = data_matrix.shape\n",
    "\n",
    "\tnum_sub_matrix = round(num_node_next_layer/num_mutiplier_parallel)\n",
    "\trow_sub_matrix = num_mutiplier_parallel\n",
    "\tcol_sub_matrix = num_node_previous_layer\n",
    "\n",
    "\tfor i in range(num_sub_matrix):\n",
    "\t\tsub_matrix = data_matrix[num_mutiplier_parallel*i:num_mutiplier_parallel*(i+1), :]\n",
    "\t\tfor j in range(col_sub_matrix):\n",
    "\t\t\tbin_final = ''\n",
    "\t\t\tfor k in range(row_sub_matrix):\n",
    "\t\t\t\tdec_data = int(sub_matrix[k][j])\n",
    "\t\t\t\tbin_data_temp = convert_int_to_binary(dec_data, bit_width)\n",
    "\t\t\t\tbin_data = bin_data_temp[-bit_width:]\n",
    "\t\t\t\tbin_final = bin_final + bin_data\n",
    "\n",
    "\t\t\tsub_bin_final_0 = convert_binary_to_hex('0b' + bin_final[0:round(len(bin_final)/2)], int(bit_width*row_sub_matrix/2))\n",
    "\t\t\tsub_bin_final_1 = convert_binary_to_hex('0b' + bin_final[-round(len(bin_final)/2):], int(bit_width*row_sub_matrix/2))\n",
    "\n",
    "\t\t\tif((j == (col_sub_matrix - 1)) and (i == num_sub_matrix - 1)):\n",
    "\t\t\t\tverilog_file.write(sub_bin_final_0[-(len(sub_bin_final_0)-2):] + ' ' + sub_bin_final_1[-(len(sub_bin_final_1)-2):])\n",
    "\t\t\telse:\n",
    "\t\t\t\tverilog_file.write(sub_bin_final_0[-(len(sub_bin_final_0)-2):] + ' ' + sub_bin_final_1[-(len(sub_bin_final_1)-2):] + '\\n')\n",
    "\tverilog_file.close()\n",
    "\n",
    "def create_txt_file_for_bias_verilog(data_matrix, num_mutiplier_parallel, bit_width, file_path):\n",
    "\tverilog_file = open(file_path, mode = 'w')\n",
    "\n",
    "\tdata_matrix = data_matrix.reshape(-1, 1)\n",
    "\n",
    "\tnum_node_next_layer, temp = data_matrix.shape\n",
    "\n",
    "\tnum_sub_matrix = round(num_node_next_layer/num_mutiplier_parallel)\n",
    "\tlen_sub_matrix = num_mutiplier_parallel\n",
    "\tfor i in range(num_sub_matrix):\n",
    "\t\tsub_matrix = data_matrix[num_mutiplier_parallel*i:num_mutiplier_parallel*(i+1)]\n",
    "\t\tbin_final = ''\n",
    "\t\tfor j in range(len_sub_matrix):\n",
    "\t\t\tdec_data = int(sub_matrix[j])\n",
    "\t\t\tbin_data_temp = convert_int_to_binary(dec_data, bit_width)\n",
    "\t\t\tbin_data = bin_data_temp[-bit_width:]\n",
    "\t\t\tbin_final = bin_final + bin_data\n",
    "\t\t\n",
    "\t\tsub_bin_final_0 = convert_binary_to_hex('0b' + bin_final[0:round(len(bin_final)/2)], int(bit_width*len_sub_matrix/2))\n",
    "\t\tsub_bin_final_1 = convert_binary_to_hex('0b' + bin_final[-round(len(bin_final)/2):], int(bit_width*len_sub_matrix/2))\n",
    "\n",
    "\t\tif(i == num_sub_matrix - 1):\n",
    "\t\t\tverilog_file.write(sub_bin_final_0[-(len(sub_bin_final_0)-2):] + ' ' + sub_bin_final_1[-(len(sub_bin_final_1)-2):])\n",
    "\t\telse:\n",
    "\t\t\tverilog_file.write(sub_bin_final_0[-(len(sub_bin_final_0)-2):] + ' ' + sub_bin_final_1[-(len(sub_bin_final_1)-2):] + '\\n')\n",
    "\tverilog_file.close()\n",
    "\n",
    "def create_txt_file_for_input_verilog(input_data, bit_width, file_path):\n",
    "    verilog_file = open(file_path, mode = 'w')\n",
    "\n",
    "    input_data = input_data.reshape(-1, 1)\n",
    "\n",
    "    num_node, temp = input_data.shape\n",
    "\n",
    "    for i in range(num_node):\n",
    "        dec_data = int(input_data[i])\n",
    "        bin_data_temp = convert_int_to_binary(dec_data, bit_width)\n",
    "        bin_data = bin_data_temp[-bit_width:]\n",
    "        hex_data = convert_binary_to_hex('0b' + bin_data, bit_width)\n",
    "        if(i != (num_node - 1)):\n",
    "            verilog_file.write(str(hex_data[-(len(hex_data)-2):]) + '\\n')\n",
    "        else:\n",
    "            verilog_file.write(str(hex_data[-(len(hex_data)-2):]))\n",
    "    verilog_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_image = 1\n",
    "min_image = -1\n",
    "\n",
    "max_filter_conv0 = 0.64\n",
    "min_filter_conv0 = -0.64\n",
    "max_filter_conv1 = 0.94\n",
    "min_filter_conv1 = -0.94\n",
    "\n",
    "max_result_conv0 = 2.67\n",
    "min_result_conv0 = -2.67\n",
    "max_result_conv1 = 8.88\n",
    "min_result_conv1 = -8.88\n",
    "\n",
    "max_bias_conv0 = 0.35\n",
    "min_bias_conv0 = -0.35\n",
    "max_bias_conv1 = 0.16\n",
    "min_bias_conv1 = -0.16\n",
    "\n",
    "max_pooling0 = 1.75\n",
    "min_pooling0 = -1.75\n",
    "max_pooling1 = 2.84\n",
    "min_pooling1 = -2.84\n",
    "\n",
    "max_w0 = 1.02\n",
    "min_w0 = -1.02\n",
    "max_w1 = 0.78\n",
    "min_w1 = -0.78\n",
    "max_w2 = 0.85\n",
    "min_w2 = -0.85\n",
    "\n",
    "max_b0 = 0.21\n",
    "min_b0 = -0.21\n",
    "max_b1 = 0.19\n",
    "min_b1 = -0.19\n",
    "max_b2 = 0.24\n",
    "min_b2 = -0.24\n",
    "\n",
    "max_in = 2.84\n",
    "min_in = -2.84\n",
    "max_hd0 = 9.6\n",
    "min_hd0 = -9.6\n",
    "max_hd1 = 12.8\n",
    "min_hd1 = -12.8\n",
    "max_out_bf_act = 23.4\n",
    "min_out_bf_act = -23.4\n",
    "max_out = 1\n",
    "min_out = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================ Bit-width ============================================\n",
    "bit_width_image = 6\n",
    "bit_width_filter_conv0 = 6\n",
    "bit_width_filter_conv1 = 6\n",
    "bit_width_conv0 = 6\n",
    "bit_width_conv1 = 6\n",
    "bit_width_bias_conv0 = 6\n",
    "bit_width_bias_conv1 = 6\n",
    "bit_width_pooling0 = 6\n",
    "bit_width_pooling1 = 6\n",
    "bit_width_input = bit_width_pooling1\n",
    "\n",
    "bit_width_w0 = 6\n",
    "bit_width_w1 = 6\n",
    "bit_width_w2 = 6\n",
    "\n",
    "bit_width_b0 = 6\n",
    "bit_width_b1 = 6\n",
    "bit_width_b2 = 6\n",
    "\n",
    "bit_width_h0 = 6\n",
    "bit_width_h1 = 6\n",
    "bit_width_bf_sigmoid = 6\n",
    "bit_width_output = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================ Max-min quantization value ============================================\n",
    "max_image_q = 2**(bit_width_image-1)-1\n",
    "min_image_q = -2**(bit_width_image-1)\n",
    "max_filter_conv0_q = 2**(bit_width_filter_conv0-1)-1\n",
    "min_filter_conv0_q = -2**(bit_width_filter_conv0-1)\n",
    "max_filter_conv1_q = 2**(bit_width_filter_conv1-1)-1\n",
    "min_filter_conv1_q = -2**(bit_width_filter_conv1-1)\n",
    "max_conv0_q = 2**(bit_width_conv0-1)-1\n",
    "min_conv0_q = -2**(bit_width_conv0-1)\n",
    "max_conv1_q = 2**(bit_width_conv1-1)-1\n",
    "min_conv1_q = -2**(bit_width_conv1-1)\n",
    "max_bias_conv0_q = 2**(bit_width_bias_conv0-1)-1\n",
    "min_bias_conv0_q = -2**(bit_width_bias_conv0-1)\n",
    "max_bias_conv1_q = 2**(bit_width_bias_conv1-1)-1\n",
    "min_bias_conv1_q = -2**(bit_width_bias_conv1-1)\n",
    "max_pooling0_q = 2**(bit_width_pooling0-1)-1\n",
    "min_pooling0_q = -2**(bit_width_pooling0-1)\n",
    "max_pooling1_q = 2**(bit_width_pooling1-1)-1\n",
    "min_pooling1_q = -2**(bit_width_pooling1-1)\n",
    "\n",
    "max_w0_q = 2**(bit_width_w0-1)-1\n",
    "min_w0_q = -2**(bit_width_w0-1)\n",
    "max_w1_q = 2**(bit_width_w1-1)-1\n",
    "min_w1_q = -2**(bit_width_w1-1)\n",
    "max_w2_q = 2**(bit_width_w2-1)-1\n",
    "min_w2_q = -2**(bit_width_w2-1)\n",
    "\n",
    "max_b0_q = 2**(bit_width_b0-1)-1\n",
    "min_b0_q = -2**(bit_width_b0-1)\n",
    "max_b1_q = 2**(bit_width_b1-1)-1\n",
    "min_b1_q = -2**(bit_width_b1-1)\n",
    "max_b2_q = 2**(bit_width_b2-1)-1\n",
    "min_b2_q = -2**(bit_width_b2-1)\n",
    "\n",
    "max_input_q = 2**(bit_width_input-1)-1\n",
    "min_input_q = -2**(bit_width_input-1)\n",
    "max_hd0_q = 2**(bit_width_h0-1)-1\n",
    "min_hd0_q = -2**(bit_width_h0-1)\n",
    "max_hd1_q = 2**(bit_width_h1-1)-1\n",
    "min_hd1_q = -2**(bit_width_h1-1)\n",
    "max_bf_sigmoid_q = 2**(bit_width_bf_sigmoid-1)-1\n",
    "min_bf_sigmoid_q = -2**(bit_width_bf_sigmoid-1)\n",
    "max_output_q = 2**(bit_width_output-1)-1\n",
    "min_output_q = -2**(bit_width_output-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================ Scale and offset for w1 & bias ============================================\n",
    "# Quantied number = (X/Scale) + Offset\n",
    "scale_image = (max_image - min_image)/(max_image_q - min_image_q)\n",
    "scale_filter_conv0 = (max_filter_conv0 - min_filter_conv0)/(max_filter_conv0_q - min_filter_conv0_q)\n",
    "scale_filter_conv1 = (max_filter_conv1 - min_filter_conv1)/(max_filter_conv1_q - min_filter_conv1_q)\n",
    "scale_conv0 = (max_result_conv0 - min_result_conv0)/(max_conv0_q - min_conv0_q)\n",
    "scale_conv1 = (max_result_conv1 - min_result_conv1)/(max_conv1_q - min_conv1_q)\n",
    "scale_bias_conv0 = (max_bias_conv0 - min_bias_conv0)/(max_bias_conv0_q - min_bias_conv0_q)\n",
    "scale_bias_conv1 = (max_bias_conv1 - min_bias_conv1)/(max_bias_conv1_q - min_bias_conv1_q)\n",
    "scale_pooling0 = (max_pooling0 - min_pooling0)/(max_pooling0_q - min_pooling0_q)\n",
    "scale_pooling1 = (max_pooling1 - min_pooling1)/(max_pooling1_q - min_pooling1_q)\n",
    "\n",
    "scale_w0 = (max_w0 - min_w0)/(max_w0_q - min_w0_q)\n",
    "scale_w1 = (max_w1 - min_w1)/(max_w1_q - min_w1_q)\n",
    "scale_w2 = (max_w2 - min_w2)/(max_w2_q - min_w2_q)\n",
    "\n",
    "scale_b0 = (max_b0 - min_b0)/(max_b0_q - min_b0_q)\n",
    "scale_b1 = (max_b1 - min_b1)/(max_b1_q - min_b1_q)\n",
    "scale_b2 = (max_b2 - min_b2)/(max_b2_q - min_b2_q)\n",
    "\n",
    "scale_input = (max_in - min_in)/(max_input_q - min_input_q)\n",
    "scale_hd0 = (max_hd0 - min_hd0)/(max_hd0_q - min_hd0_q)\n",
    "scale_hd1 = (max_hd1 - min_hd1)/(max_hd1_q - min_hd1_q)\n",
    "scale_bf_sigmoid = (max_out_bf_act - min_out_bf_act)/(max_bf_sigmoid_q - min_bf_sigmoid_q)\n",
    "scale_output = (max_out - min_out)/(max_output_q - min_output_q)\n",
    "\n",
    "offset_image = round((max_image*min_image_q - min_image*max_image_q)/(max_image - min_image))\n",
    "offset_filter_conv0 = round((max_filter_conv0*min_filter_conv0_q - min_filter_conv0*max_filter_conv0_q)/(max_filter_conv0 - min_filter_conv0))\n",
    "offset_filter_conv1 = round((max_filter_conv1*min_filter_conv1_q - min_filter_conv1*max_filter_conv1_q)/(max_filter_conv1 - min_filter_conv1))\n",
    "offset_conv0 = round((max_result_conv0*min_conv0_q - min_result_conv0*max_conv0_q)/(max_result_conv0 - min_result_conv0))\n",
    "offset_conv1 = round((max_result_conv1*min_conv1_q - min_result_conv1*max_conv1_q)/(max_result_conv1 - min_result_conv1))\n",
    "offset_bias_conv0 = round((max_bias_conv0*min_bias_conv0_q - min_bias_conv0*max_bias_conv0_q)/(max_bias_conv0 - min_bias_conv0))\n",
    "offset_bias_conv1 = round((max_bias_conv1*min_bias_conv1_q - min_bias_conv1*max_bias_conv1_q)/(max_bias_conv1 - min_bias_conv1))\n",
    "offset_pooling0 = round((max_pooling0*min_pooling0_q - min_pooling0*max_pooling0_q)/(max_pooling0 - min_pooling0))\n",
    "offset_pooling1 = round((max_pooling1*min_pooling1_q - min_pooling1*max_pooling1_q)/(max_pooling1 - min_pooling1))\n",
    "\n",
    "offset_w0 = round((max_w0*min_w0_q - min_w0*max_w0_q)/(max_w0 - min_w0))\n",
    "offset_w1 = round((max_w1*min_w1_q - min_w1*max_w1_q)/(max_w1 - min_w1))\n",
    "offset_w2 = round((max_w2*min_w2_q - min_w2*max_w2_q)/(max_w2 - min_w2))\n",
    "\n",
    "offset_b0 = round((max_b0*min_b0_q - min_b0*max_b0_q)/(max_b0 - min_b0))\n",
    "offset_b1 = round((max_b1*min_b1_q - min_b1*max_b1_q)/(max_b1 - min_b1))\n",
    "offset_b2 = round((max_b2*min_b2_q - min_b2*max_b2_q)/(max_b2 - min_b2))\n",
    "\n",
    "offset_input = round((max_in*min_input_q - min_in*max_input_q)/(max_in - min_in))\n",
    "offset_hd0 = round((max_hd0*min_hd0_q - min_hd0*max_hd0_q)/(max_hd0 - min_hd0))\n",
    "offset_hd1 = round((max_hd1*min_hd1_q - min_hd1*max_hd1_q)/(max_hd1 - min_hd1))\n",
    "offset_bf_sigmoid = round((max_out_bf_act*min_bf_sigmoid_q - min_out_bf_act*max_bf_sigmoid_q)/(max_out_bf_act - min_out_bf_act))\n",
    "offset_output = round((max_out*min_output_q - min_out*max_output_q)/(max_out - min_out))\n",
    "\n",
    "# ============================================ Scale for quantize ============================================\n",
    "# X * Scale \n",
    "scale_calculate_convolution_0 = (scale_image*scale_filter_conv0)/scale_conv0\n",
    "scale_for_pooling0 = scale_conv0/scale_pooling0\n",
    "scale_calculate_convolution_1 = (scale_pooling0*scale_filter_conv1)/scale_conv1\n",
    "scale_for_pooling1 = scale_conv1/scale_pooling1\n",
    "\n",
    "scale_for_W0_x_Flatten = (scale_input*scale_w0)/scale_hd0\n",
    "scale_for_W1_x_hd0 = (scale_hd0*scale_w1)/scale_hd1\n",
    "scale_for_W2_x_hd1 = (scale_hd1*scale_w2)/scale_bf_sigmoid\n",
    "\n",
    "scale_for_B0_conv = scale_bias_conv0/scale_conv0\n",
    "scale_for_B1_conv = scale_bias_conv1/scale_conv1\n",
    "\n",
    "scale_for_B0_fc = scale_b0/scale_hd0\n",
    "scale_for_B1_fc = scale_b1/scale_hd1\n",
    "scale_for_B2_fc = scale_b2/scale_bf_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mutiplier_parallel_for_acc_0 = 120\n",
    "num_mutiplier_parallel_for_acc_1 = 28\n",
    "num_mutiplier_parallel_for_acc_2 = 2\n",
    "\n",
    "Conv0_shape = (5, 5, 1, 6)\n",
    "Conv1_shape = (5, 5, 6, 16)\n",
    "\n",
    "image_path_verilog = '../../Weight_and_bias_of_model_verilog/image.txt'\n",
    "Conv0_path_verilog = '../../Weight_and_bias_of_model_verilog/Conv0.txt'\n",
    "Bias_conv0_path_verilog = '../../Weight_and_bias_of_model_verilog/Bias_conv0.txt'\n",
    "Conv1_path_verilog = '../../Weight_and_bias_of_model_verilog/Conv1.txt'\n",
    "Bias_conv1_path_verilog = '../../Weight_and_bias_of_model_verilog/Bias_conv1.txt'\n",
    "\n",
    "input_file_path_verilog = '../../Weight_and_bias_of_model_verilog/Input.txt'\n",
    "\n",
    "W0_path_verilog = '../../Weight_and_bias_of_model_verilog/W1.txt'\n",
    "W1_path_verilog = '../../Weight_and_bias_of_model_verilog/W2.txt'\n",
    "W2_path_verilog = '../../Weight_and_bias_of_model_verilog/W3.txt'\n",
    "\n",
    "B0_path_verilog = '../../Weight_and_bias_of_model_verilog/B1.txt'\n",
    "B1_path_verilog = '../../Weight_and_bias_of_model_verilog/B2.txt'\n",
    "B2_path_verilog = '../../Weight_and_bias_of_model_verilog/B3.txt'\n",
    "\n",
    "Conv0_path = '../../Weight_and_bias_of_model_python/Conv0.txt'\n",
    "Bias_conv0_path = '../../Weight_and_bias_of_model_python/Bias_conv0.txt'\n",
    "Conv1_path = '../../Weight_and_bias_of_model_python/Conv1.txt'\n",
    "Bias_conv1_path = '../../Weight_and_bias_of_model_python/Bias_conv1.txt'\n",
    "\n",
    "W0_path = '../../Weight_and_bias_of_model_python/W0.txt'\n",
    "W1_path = '../../Weight_and_bias_of_model_python/W1.txt'\n",
    "W2_path = '../../Weight_and_bias_of_model_python/W2.txt'\n",
    "\n",
    "B0_path = '../../Weight_and_bias_of_model_python/B0.txt'\n",
    "B1_path = '../../Weight_and_bias_of_model_python/B1.txt'\n",
    "B2_path = '../../Weight_and_bias_of_model_python/B2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"../../Dataset/Data_test_10000.pickle\",\"rb\")\n",
    "image_test = pickle.load(pickle_in)\n",
    "pickle_in = open(\"../../Dataset/Label_test_10000.pickle\",\"rb\")\n",
    "label_test = pickle.load(pickle_in)\n",
    "\n",
    "Conv0 = Load_Conv_and_W_and_Bias(Conv0_path, 1, Conv0_shape)\n",
    "Bias_conv0 = Load_Conv_and_W_and_Bias(Bias_conv0_path, 2, 0)\n",
    "Conv1 = Load_Conv_and_W_and_Bias(Conv1_path, 1, Conv1_shape)\n",
    "Bias_conv1 = Load_Conv_and_W_and_Bias(Bias_conv1_path, 2, 0)\n",
    "\n",
    "W0 = Load_Conv_and_W_and_Bias(W0_path, 1, 0)\n",
    "W1 = Load_Conv_and_W_and_Bias(W1_path, 1, 0)\n",
    "W2 = Load_Conv_and_W_and_Bias(W2_path, 1, 0)\n",
    "\n",
    "B0 = Load_Conv_and_W_and_Bias(B0_path, 2, 0)\n",
    "B1 = Load_Conv_and_W_and_Bias(B1_path, 2, 0)\n",
    "B2 = Load_Conv_and_W_and_Bias(B2_path, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv0_quant = Quantization2d(Conv0, min_filter_conv0_q, max_filter_conv0_q, scale_filter_conv0, 0)\n",
    "Bias_conv0_quant = Quantization2d(Bias_conv0, min_bias_conv0_q, max_bias_conv0_q, scale_bias_conv0, 0)\n",
    "Conv1_quant = Quantization2d(Conv1, min_filter_conv1_q, max_filter_conv1_q, scale_filter_conv1, 0)\n",
    "Bias_conv1_quant = Quantization2d(Bias_conv1, min_bias_conv1_q, max_bias_conv1_q, scale_bias_conv1, 0)\n",
    "\n",
    "W0_quant = Quantization2d(W0, min_w0_q, max_w0_q, scale_w0, 0)\n",
    "W1_quant = Quantization2d(W1, min_w1_q, max_w1_q, scale_w1, 0)\n",
    "W2_quant = Quantization2d(W2, min_w2_q, max_w2_q, scale_w2, 0)\n",
    "\n",
    "B0_quant = Quantization2d(B0, min_b0_q, max_b0_q, scale_b0, 0)\n",
    "B1_quant = Quantization2d(B1, min_b1_q, max_b1_q, scale_b1, 0)\n",
    "B2_quant = Quantization2d(B2, min_b2_q, max_b2_q, scale_b2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv0 = Load_Conv_and_W_and_Bias(Conv0_path, 0, Conv0_shape)\n",
    "Bias_conv0 = Load_Conv_and_W_and_Bias(Bias_conv0_path, 2, 0)\n",
    "Conv1 = Load_Conv_and_W_and_Bias(Conv1_path, 0, Conv1_shape)\n",
    "Bias_conv1 = Load_Conv_and_W_and_Bias(Bias_conv1_path, 2, 0)\n",
    "\n",
    "Conv0_quant = Quantization4d(Conv0, min_filter_conv0_q, max_filter_conv0_q, scale_filter_conv0, 0)\n",
    "Bias_conv0_quant = Quantization2d(Bias_conv0, min_bias_conv0_q, max_bias_conv0_q, scale_bias_conv0, 0)\n",
    "Conv1_quant = Quantization4d(Conv1, min_filter_conv1_q, max_filter_conv1_q, scale_filter_conv1, 0)\n",
    "Bias_conv1_quant = Quantization2d(Bias_conv1, min_bias_conv1_q, max_bias_conv1_q, scale_bias_conv1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input\n [[0.]\n [2.]\n [0.]\n [2.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [3.]\n [1.]\n [0.]\n [5.]\n [0.]\n [0.]\n [1.]\n [2.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [2.]\n [0.]\n [0.]\n [0.]\n [2.]\n [0.]\n [2.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [2.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [2.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [2.]\n [2.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [2.]\n [0.]\n [2.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [2.]\n [0.]\n [4.]\n [0.]\n [2.]\n [3.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [2.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [2.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [4.]\n [0.]\n [2.]\n [0.]\n [0.]\n [2.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [2.]\n [0.]\n [0.]\n [0.]\n [3.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [2.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [2.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [4.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [4.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [6.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [5.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [5.]\n [0.]\n [0.]\n [2.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [2.]\n [0.]\n [0.]\n [1.]\n [0.]\n [2.]\n [3.]\n [0.]\n [2.]\n [0.]\n [2.]\n [2.]\n [1.]\n [0.]\n [0.]\n [0.]\n [5.]\n [0.]\n [0.]\n [4.]\n [0.]\n [0.]\n [5.]\n [0.]\n [1.]\n [0.]\n [4.]\n [2.]\n [2.]\n [0.]\n [0.]\n [0.]\n [4.]\n [0.]\n [0.]\n [4.]\n [1.]\n [0.]\n [5.]\n [2.]\n [1.]\n [0.]\n [2.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [5.]\n [0.]\n [0.]\n [2.]\n [1.]\n [0.]\n [3.]]\nhd0\n [[0.]\n [0.]\n [2.]\n [1.]\n [4.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [3.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [4.]\n [0.]\n [2.]\n [0.]\n [0.]\n [1.]\n [0.]\n [2.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [4.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [2.]\n [0.]\n [1.]\n [0.]\n [0.]\n [3.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [4.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [2.]\n [0.]\n [1.]\n [0.]\n [2.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [2.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [2.]\n [1.]\n [5.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]]\nhd1\n [[0.]\n [0.]\n [3.]\n [4.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [5.]\n [0.]\n [3.]\n [2.]\n [0.]\n [3.]\n [3.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [2.]\n [0.]\n [0.]\n [4.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [4.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [2.]\n [0.]\n [0.]\n [4.]\n [0.]\n [0.]\n [0.]\n [0.]\n [4.]\n [3.]\n [0.]\n [0.]\n [6.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [2.]\n [2.]\n [0.]\n [0.]\n [0.]\n [3.]\n [0.]\n [1.]\n [3.]\n [0.]\n [0.]\n [2.]]\noutput\n [[0.66846939]\n [0.66846939]\n [1.        ]\n [0.33153061]\n [0.19755102]\n [0.03306122]\n [0.33153061]\n [0.        ]\n [0.33153061]\n [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "quatized_image = Quantization3d(image_test[1], min_input_q, max_input_q, scale_input, 0)\n",
    "\n",
    "result, input_for_fc = Feed_forward_Lenet_5_quantization(quatized_image, Conv0_quant, Bias_conv0_quant, Conv1_quant, Bias_conv1_quant, W0_quant, W1_quant, W2_quant, B0_quant, B1_quant, B2_quant, scale_calculate_convolution_0, scale_for_pooling0, scale_calculate_convolution_1, scale_for_pooling1, scale_for_W0_x_Flatten, scale_for_W1_x_hd0, scale_for_W2_x_hd1, scale_output, scale_for_B0_conv, scale_for_B1_conv, scale_for_B0_fc, scale_for_B1_fc, scale_for_B2_fc, max_conv0_q, min_conv0_q, max_conv1_q, min_conv1_q, max_pooling0_q, min_pooling0_q, max_pooling1_q, min_pooling1_q, max_hd0_q, min_hd0_q, max_hd1_q, min_hd1_q, max_bf_sigmoid_q, min_bf_sigmoid_q, scale_bf_sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 2\n"
     ]
    }
   ],
   "source": [
    "print(result, label_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== Input ==========================\n",
    "create_txt_file_for_input_verilog(input_for_fc, bit_width_input, input_file_path_verilog)\n",
    "\n",
    "create_txt_file_for_input_verilog(quatized_image, bit_width_image, image_path_verilog)\n",
    "\n",
    "create_txt_file_for_input_verilog(Conv0_quant, bit_width_filter_conv0, Conv0_path_verilog)\n",
    "create_txt_file_for_input_verilog(Bias_conv0_quant, bit_width_bias_conv0, Bias_conv0_path_verilog)\n",
    "create_txt_file_for_input_verilog(Conv1_quant, bit_width_filter_conv1, Conv1_path_verilog)\n",
    "create_txt_file_for_input_verilog(Bias_conv1_quant, bit_width_bias_conv1, Bias_conv1_path_verilog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_txt_file_for_w_verilog(W0_quant, num_mutiplier_parallel_for_acc_0, bit_width_w0, W0_path_verilog)\n",
    "create_txt_file_for_w_verilog(W1_quant, num_mutiplier_parallel_for_acc_1, bit_width_w1, W1_path_verilog)\n",
    "create_txt_file_for_w_verilog(W2_quant, num_mutiplier_parallel_for_acc_2, bit_width_w2, W2_path_verilog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_txt_file_for_bias_verilog(B0_quant, num_mutiplier_parallel_for_acc_0, bit_width_b0, B0_path_verilog)\n",
    "create_txt_file_for_bias_verilog(B1_quant, num_mutiplier_parallel_for_acc_1, bit_width_b1, B1_path_verilog)\n",
    "create_txt_file_for_bias_verilog(B2_quant, num_mutiplier_parallel_for_acc_2, bit_width_b2, B2_path_verilog)"
   ]
  }
 ]
}