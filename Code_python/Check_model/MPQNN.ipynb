{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd05c3f60802471494b60a2e66fd4ecd6fdf92055af140e68ee192931e29e235180",
   "display_name": "Python 3.8.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Quantization2d(data_arr, min_q, max_q, scale, offset):\n",
    "\trow, col = data_arr.shape\n",
    "\tdata_quantization = np.ones((row, col))\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\tdata_quantization[i][j] = round((data_arr[i][j] / scale) + offset)\n",
    "\n",
    "\t\t\tif(data_quantization[i][j] > max_q):\n",
    "\t\t\t\tdata_quantization[i][j] = max_q\n",
    "\t\t\telif(data_quantization[i][j] < min_q):\n",
    "\t\t\t\tdata_quantization[i][j] = min_q\n",
    "\n",
    "\treturn data_quantization\n",
    "\n",
    "def Quantization3d(data, min_q, max_q, scale, offset):\n",
    "\trow, col, num_filter = data.shape\n",
    "\tdata_quantization = np.ones((row, col, num_filter))\n",
    "\n",
    "\tfor i in range(num_filter):\n",
    "\t\tdata_quantization[:, :, i] = Quantization2d(data[:, :, i], min_q, max_q, scale, offset)\n",
    "        \n",
    "\treturn data_quantization\n",
    "\n",
    "def Quantization4d(data, min_q, max_q, scale, offset):\n",
    "\trow, col, num_subfilter, num_filter = data.shape\n",
    "\tdata_quantization = np.ones((row, col, num_subfilter, num_filter))\n",
    "\n",
    "\tfor i in range(num_filter):\n",
    "\t\tdata_quantization[:, :, :, i] = Quantization3d(data[:, :, :, i], min_q, max_q, scale, offset)\n",
    "\n",
    "\treturn data_quantization\n",
    "\n",
    "def Dequantization2d(data_arr, scale, offset):\n",
    "\trow, col = data_arr.shape\n",
    "\tdata_dequantization = np.ones((row, col))\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\tdata_dequantization[i][j] = (data_arr[i][j] - offset) * scale\n",
    "\n",
    "\treturn data_dequantization\n",
    "\n",
    "def\tRound_quantization2d(data_arr, min_q, max_q):\n",
    "\trow, col = data_arr.shape\n",
    "\tdata_round_quantization = np.ones((row, col))\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\tdata = float(data_arr[i][j])\n",
    "\t\t\tdata_round_quantization[i][j] = round(data)\n",
    "\n",
    "\t\t\tif(data_round_quantization[i][j] > max_q):\n",
    "\t\t\t\tdata_round_quantization[i][j] = max_q\n",
    "\t\t\telif(data_round_quantization[i][j] < min_q):\n",
    "\t\t\t\tdata_round_quantization[i][j] = min_q\n",
    "\n",
    "\treturn data_round_quantization\n",
    "\n",
    "def\tRound_quantization3d(data, min_q, max_q):\n",
    "\trow, col, num_filter = data.shape\n",
    "\tdata_round_quantization = np.ones((row, col, num_filter))\n",
    "\n",
    "\tfor i in range(num_filter):\n",
    "\t\tdata_round_quantization[:, :, i] = Round_quantization2d(data[:, :, i], min_q, max_q)\n",
    "\n",
    "\treturn data_round_quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(data):\n",
    "\tif(data >= 0):\n",
    "\t\treturn data\n",
    "\telse: \n",
    "\t\treturn 0\n",
    "def ReLU_2d(data):\n",
    "\trow, col = data.shape\n",
    "\tdata_ret = np.zeros((row, col))\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\tdata_ret[i, j] = ReLU(data[i, j])\n",
    "\t\n",
    "\treturn data_ret\n",
    "\n",
    "def ReLU_3d(data):\n",
    "\trow, col, num_filter = data.shape\n",
    "\tdata_ret = np.zeros((row, col, num_filter))\n",
    "\n",
    "\tfor i in range(num_filter):\n",
    "\t\tdata_ret[:, :, i] = ReLU_2d(data[:, :, i])\n",
    "\n",
    "\treturn data_ret\n",
    "\n",
    "def Sigmoid(data):\n",
    "    if(data < -4):\n",
    "        return 0\n",
    "    elif((-4 <= data) and (data < 0)):\n",
    "        return (1/2)*((1+(data/4))**2)\n",
    "    elif((0 <= data) and (data < 4)):\n",
    "        return 1-(1/2)*((1-(data/4))**2)\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def Sigmoid_2d(data):\n",
    "\trow, col = data.shape\n",
    "\tdata_ret = np.zeros((row, col))\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\tdata_ret[i, j] = Sigmoid(data[i, j])\n",
    "\t\n",
    "\treturn data_ret\n",
    "\n",
    "def max_pooling(matrix):\n",
    "\trow, col = matrix.shape\n",
    "\n",
    "\tmax_value = 0\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\tif(matrix[i][j] > max_value):\n",
    "\t\t\t\tmax_value = matrix[i][j]\n",
    "\treturn max_value\n",
    "\n",
    "def avg_pooling(matrix):\n",
    "\trow, col = matrix.shape\n",
    "\n",
    "\tsum_value = 0\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\t\tsum_value = sum_value + matrix[i][j]\n",
    "\n",
    "\tavg_value = sum_value/(col*row)\n",
    "\treturn avg_value\n",
    "\n",
    "def median_pooling(matrix):\n",
    "\trow, col = matrix.shape\n",
    "\n",
    "\tmatrix = np.reshape(matrix, (1, row*col))\n",
    "\n",
    "\tmatrix = np.sort(matrix)\n",
    "\n",
    "\tnum_element = row*col\n",
    "\n",
    "\tmedian_index = int(num_element/2)\n",
    "\n",
    "\treturn matrix[0, median_index]\n",
    "\n",
    "def pooling_2d(x_matrix, dim_filter, stride, padding, pooling_option):\n",
    "\tx_row, x_col = x_matrix.shape\n",
    "\tfilter_row, filter_col = dim_filter\n",
    "\n",
    "\ty_row = int(((x_row - filter_row + 2*padding)/stride) + 1)\n",
    "\ty_col = int(((x_col - filter_col + 2*padding)/stride) + 1)\n",
    "\n",
    "\tif(padding >= 1):\n",
    "\t\ttemp_matrix = x_matrix\n",
    "\t\tx_row = x_row + 2*padding\n",
    "\t\tx_col = x_col + 2*padding\n",
    "\t\tx_matrix = np.zeros((x_row, x_col))\n",
    "\t\tx_matrix[0+padding:x_row-padding, 0+padding:x_col-padding] = temp_matrix\n",
    "\n",
    "\ty = np.zeros((y_row, y_col))\n",
    "\n",
    "\tindex_row = np.arange(start=0, stop=x_row-filter_row+1, step=stride)\n",
    "\tindex_col = np.arange(start=0, stop=x_col-filter_col+1, step=stride)\n",
    "\tfor i in range(len(index_row)):\n",
    "\t\tfor j in range(len(index_col)):\n",
    "\t\t\tsub_matrix = x_matrix[index_row[i]:index_row[i]+filter_row, index_col[j]:index_col[j]+filter_col]\n",
    "\n",
    "\t\t\tif(pooling_option == 0):\n",
    "\t\t\t\ty[i, j] = max_pooling(sub_matrix)\n",
    "\t\t\telif(pooling_option == 1):\n",
    "\t\t\t\ty[i, j] = avg_pooling(sub_matrix)\n",
    "\t\t\telse:\n",
    "\t\t\t\ty[i, j] = median_pooling(sub_matrix)\n",
    "\t\t\t\n",
    "\treturn y\n",
    "\n",
    "def pooling_3d(x_matrix, dim_filter, stride, padding, pooling_option):\n",
    "\tx_row, x_col, num_filter = x_matrix.shape\n",
    "\tfilter_row, filter_col = dim_filter\n",
    "\n",
    "\ty_row = int(((x_row - filter_row + 2*padding)/stride) + 1)\n",
    "\ty_col = int(((x_col - filter_col + 2*padding)/stride) + 1)\n",
    "\n",
    "\ty = np.zeros((y_row, y_col, num_filter))\n",
    "\n",
    "\tfor i in range(num_filter):\n",
    "\t\tif(pooling_option == 0):\n",
    "\t\t\ty[:, :, i] = pooling_2d(x_matrix[:, :, i], dim_filter, stride, padding, pooling_option)\n",
    "\t\telif(pooling_option == 1):\n",
    "\t\t\ty[:, :, i] = pooling_2d(x_matrix[:, :, i], dim_filter, stride, padding, pooling_option)\n",
    "\t\telse:\n",
    "\t\t\ty[:, :, i] = pooling_2d(x_matrix[:, :, i], dim_filter, stride, padding, pooling_option)\n",
    "\n",
    "\treturn y\n",
    "\n",
    "def conv(x_matrix, filter_matrix):\n",
    "\trow, col = x_matrix.shape\n",
    "\ty = np.zeros((row, col))\n",
    "\n",
    "\tfor i in range(row):\n",
    "\t\tfor j in range(col):\n",
    "\t\t\ty[i, j] = x_matrix[i, j]*filter_matrix[i, j]\n",
    "\t\n",
    "\treturn y.sum()\n",
    "\t\n",
    "def conv2(x_matrix, filter_matrix, stride, padding):\n",
    "\tx_row, x_col = x_matrix.shape\n",
    "\tfilter_row, filter_col = filter_matrix.shape \n",
    "\n",
    "\ty_row = int(((x_row - filter_row + 2*padding)/stride) + 1)\n",
    "\ty_col = int(((x_col - filter_col + 2*padding)/stride) + 1)\n",
    "\n",
    "\tif(padding >= 1):\n",
    "\t\ttemp_matrix = x_matrix\n",
    "\t\tx_row = x_row + 2*padding\n",
    "\t\tx_col = x_col + 2*padding\n",
    "\t\tx_matrix = np.zeros((x_row, x_col))\n",
    "\t\tx_matrix[0+padding:x_row-padding, 0+padding:x_col-padding] = temp_matrix\n",
    "\n",
    "\ty = np.zeros((y_row, y_col))\n",
    "\n",
    "\tindex_row = np.arange(start=0, stop=x_row-filter_row+1, step=stride)\n",
    "\tindex_col = np.arange(start=0, stop=x_col-filter_col+1, step=stride)\n",
    "\tfor i in range(len(index_row)):\n",
    "\t\tfor j in range(len(index_col)):\n",
    "\t\t\tsub_matrix = x_matrix[index_row[i]:index_row[i]+filter_row, index_col[j]:index_col[j]+filter_col]\n",
    "\t\t\ty[i, j] = conv(sub_matrix, filter_matrix)\n",
    "\treturn y\n",
    "\n",
    "def conv3(x_matrix, filter_matrix, stride, padding):\n",
    "\trow_filter, col_filter, num_sub_filter, num_filter = filter_matrix.shape \t\t\t\t# row_filter_matrix = col_filter_matrix\n",
    "\trow_x, col_x, num_x = x_matrix.shape \t\t\t\t\t\t\t\t\t\t\t\t\t# row_x = col_x\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# num_sub_filter = num_x\n",
    "\ty_row = int(((row_x - row_filter + 2*padding)/stride) + 1)\n",
    "\ty_col = int(((col_x - col_filter + 2*padding)/stride) + 1)\n",
    "\t\n",
    "\tdata = np.zeros((y_row, y_col, num_filter))\n",
    "\tfor i in range(num_filter):\n",
    "\t\tdata_temp = np.zeros((y_row, y_col))\n",
    "\n",
    "\t\tfor j in range(num_sub_filter):\n",
    "\t\t\tdata_temp = data_temp + conv2(x_matrix[:, :, j], filter_matrix[:, :, j, i], stride, padding)\n",
    "\t\t\n",
    "\t\tdata[:, :, i] = data_temp\n",
    "\n",
    "\treturn data\n",
    "\n",
    "def add_bias_after_conv(conv_result, bias):\n",
    "\trow, col, num_filter = conv_result.shape\n",
    "\n",
    "\tfor i in range(num_filter):\n",
    "\t\tconv_result[:, :, i] = conv_result[:, :, i] + bias[i, :]\n",
    "\n",
    "\treturn conv_result\n",
    "\n",
    "def Feed_forward_Lenet_5_quantization(image, conv0_filter, bias_conv0, conv1_filter, bias_conv1, W0, W1, W2, B0, B1, B2, scale_calculate_convolution_0, scale_for_pooling0, scale_calculate_convolution_1, scale_for_pooling1, scale_for_W0_x_Flatten, scale_for_W1_x_hd0, scale_for_W2_x_hd1, scale_output, scale_for_B0_conv, scale_for_B1_conv, scale_for_B0_fc, scale_for_B1_fc, scale_for_B2_fc, max_conv0_q, min_conv0_q, max_conv1_q, min_conv1_q, max_pooling0_q, min_pooling0_q, max_pooling1_q, min_pooling1_q, max_hd0_q, min_hd0_q, max_hd1_q, min_hd1_q, max_bf_sigmoid_q, min_bf_sigmoid_q, scale_bf_sigmoid):\n",
    "\tstride = 1\n",
    "\tpadding = 0\n",
    "\tstride_pooling = 2\n",
    "\tpadding_pooling = 0\n",
    "\tpooling_option = 1\n",
    "\n",
    "\t# Convolution layer 0\n",
    "\tconv0_result = conv3(image, conv0_filter, stride, padding)\n",
    "\tadd_bias_after_conv0_bf_relu = Round_quantization3d(add_bias_after_conv(conv0_result*scale_calculate_convolution_0, bias_conv0*scale_for_B0_conv), min_conv0_q, max_conv0_q)\n",
    "\tadd_bias_after_conv0 = ReLU_3d(add_bias_after_conv0_bf_relu)\n",
    "\tpooling0 = Round_quantization3d(pooling_3d(add_bias_after_conv0, (2, 2), stride_pooling, padding_pooling, pooling_option)*scale_for_pooling0, min_pooling0_q, max_pooling0_q)\n",
    "\n",
    "\t# Convolution layer 1\n",
    "\tconv1_result = conv3(pooling0, conv1_filter, stride, padding)\n",
    "\tadd_bias_after_conv1_bf_relu = Round_quantization3d(add_bias_after_conv(conv1_result*scale_calculate_convolution_1, bias_conv1*scale_for_B1_conv), min_conv1_q, max_conv1_q)\n",
    "\tadd_bias_after_conv1 = ReLU_3d(add_bias_after_conv1_bf_relu)\n",
    "\tpooling1 = Round_quantization3d(pooling_3d(add_bias_after_conv1, (2, 2), stride_pooling, padding_pooling, pooling_option)*scale_for_pooling1, min_pooling1_q, max_pooling1_q)\n",
    "\t\n",
    "\t# Fully connected\n",
    "\tflatten = pooling1.reshape(-1, 1)\n",
    "\thd_layer0_bf_relu = Round_quantization2d(np.dot(W0, flatten)*scale_for_W0_x_Flatten + B0*scale_for_B0_fc, min_hd0_q, max_hd0_q)\n",
    "\thd_layer0 = ReLU_2d(hd_layer0_bf_relu)\n",
    "\thd_layer1_bf_relu = Round_quantization2d(np.dot(W1, hd_layer0)*scale_for_W1_x_hd0 + B1*scale_for_B1_fc, min_hd1_q, max_hd1_q) \n",
    "\thd_layer1 = ReLU_2d(hd_layer1_bf_relu)\n",
    "\toutput_bf_sigmoid = Round_quantization2d(np.dot(W2, hd_layer1)*scale_for_W2_x_hd1 + B2*scale_for_B2_fc, min_bf_sigmoid_q, max_bf_sigmoid_q)\n",
    "\toutput = Sigmoid_2d(Dequantization2d(output_bf_sigmoid, scale_bf_sigmoid, 0))\n",
    "\n",
    "\treturn np.argmax(output)\n",
    "\n",
    "def Load_Conv_and_W_and_Bias(data_path, load_option, conv_shape):\n",
    "\ttemp_data = np.loadtxt(data_path)\n",
    "\n",
    "\tif(load_option == 0): # ================== Conv\n",
    "\t\trow, col, num_sub_filter, num_filter = conv_shape # row = col\n",
    "\t\tdata = np.zeros((row, col, num_sub_filter, num_filter))\n",
    "\n",
    "\t\tindex = 0 # maximum index = num_sub_filter * num_filter\n",
    "\t\tfor m in range(num_filter):\n",
    "\t\t\tfor k in range(num_sub_filter):\n",
    "\t\t\t\tdata[:, :, k, m] = temp_data[index * row: index * row + 5, :]\n",
    "\t\t\t\tindex = index + 1\n",
    "\t\treturn data\n",
    "\telif(load_option == 1): # ================ W\n",
    "\t\treturn temp_data\n",
    "\telse: # ============================= Bias\n",
    "\t\treturn temp_data.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv0_shape = (5, 5, 1, 6)\n",
    "Conv1_shape = (5, 5, 6, 16)\n",
    "\n",
    "Conv0_path = '../../Weight_and_bias_of_model_python/Conv0.txt'\n",
    "Bias_conv0_path = '../../Weight_and_bias_of_model_python/Bias_conv0.txt'\n",
    "Conv1_path = '../../Weight_and_bias_of_model_python/Conv1.txt'\n",
    "Bias_conv1_path = '../../Weight_and_bias_of_model_python/Bias_conv1.txt'\n",
    "\n",
    "W0_path = '../../Weight_and_bias_of_model_python/W0.txt'\n",
    "W1_path = '../../Weight_and_bias_of_model_python/W1.txt'\n",
    "W2_path = '../../Weight_and_bias_of_model_python/W2.txt'\n",
    "\n",
    "B0_path = '../../Weight_and_bias_of_model_python/B0.txt'\n",
    "B1_path = '../../Weight_and_bias_of_model_python/B1.txt'\n",
    "B2_path = '../../Weight_and_bias_of_model_python/B2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"../../Dataset/Data_test_10000.pickle\",\"rb\")\n",
    "image_test = pickle.load(pickle_in)\n",
    "pickle_in = open(\"../../Dataset/Label_test_10000.pickle\",\"rb\")\n",
    "label_test = pickle.load(pickle_in)\n",
    "\n",
    "Conv0 = Load_Conv_and_W_and_Bias(Conv0_path, 0, Conv0_shape)\n",
    "Bias_conv0 = Load_Conv_and_W_and_Bias(Bias_conv0_path, 2, 0)\n",
    "Conv1 = Load_Conv_and_W_and_Bias(Conv1_path, 0, Conv1_shape)\n",
    "Bias_conv1 = Load_Conv_and_W_and_Bias(Bias_conv1_path, 2, 0)\n",
    "\n",
    "W0 = Load_Conv_and_W_and_Bias(W0_path, 1, 0)\n",
    "W1 = Load_Conv_and_W_and_Bias(W1_path, 1, 0)\n",
    "W2 = Load_Conv_and_W_and_Bias(W2_path, 1, 0)\n",
    "\n",
    "B0 = Load_Conv_and_W_and_Bias(B0_path, 2, 0)\n",
    "B1 = Load_Conv_and_W_and_Bias(B1_path, 2, 0)\n",
    "B2 = Load_Conv_and_W_and_Bias(B2_path, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_image = 1\n",
    "min_image = -1\n",
    "\n",
    "max_filter_conv0 = 0.64\n",
    "min_filter_conv0 = -0.64\n",
    "max_filter_conv1 = 0.94\n",
    "min_filter_conv1 = -0.94\n",
    "\n",
    "max_result_conv0 = 2.67\n",
    "min_result_conv0 = -2.67\n",
    "max_result_conv1 = 8.88\n",
    "min_result_conv1 = -8.88\n",
    "\n",
    "max_bias_conv0 = 0.35\n",
    "min_bias_conv0 = -0.35\n",
    "max_bias_conv1 = 0.16\n",
    "min_bias_conv1 = -0.16\n",
    "\n",
    "max_pooling0 = 1.75\n",
    "min_pooling0 = -1.75\n",
    "max_pooling1 = 2.84\n",
    "min_pooling1 = -2.84\n",
    "\n",
    "max_w0 = 1.02\n",
    "min_w0 = -1.02\n",
    "max_w1 = 0.78\n",
    "min_w1 = -0.78\n",
    "max_w2 = 0.85\n",
    "min_w2 = -0.85\n",
    "\n",
    "max_b0 = 0.21\n",
    "min_b0 = -0.21\n",
    "max_b1 = 0.19\n",
    "min_b1 = -0.19\n",
    "max_b2 = 0.24\n",
    "min_b2 = -0.24\n",
    "\n",
    "max_in = 2.84\n",
    "min_in = -2.84\n",
    "max_hd0 = 9.6\n",
    "min_hd0 = -9.6\n",
    "max_hd1 = 12.8\n",
    "min_hd1 = -12.8\n",
    "max_out_bf_act = 23.4\n",
    "min_out_bf_act = -23.4\n",
    "max_out = 1\n",
    "min_out = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================ Bit-width ============================================\n",
    "bit_width_image = 6\n",
    "bit_width_filter_conv0 = 6\n",
    "bit_width_filter_conv1 = 6\n",
    "bit_width_conv0 = 6\n",
    "bit_width_conv1 = 6\n",
    "bit_width_bias_conv0 = 6\n",
    "bit_width_bias_conv1 = 6\n",
    "bit_width_pooling0 = 6\n",
    "bit_width_pooling1 = 6\n",
    "bit_width_input = bit_width_pooling1\n",
    "\n",
    "bit_width_w0 = 6\n",
    "bit_width_w1 = 6\n",
    "bit_width_w2 = 6\n",
    "\n",
    "bit_width_b0 = 6\n",
    "bit_width_b1 = 6\n",
    "bit_width_b2 = 6\n",
    "\n",
    "bit_width_h0 = 6\n",
    "bit_width_h1 = 6\n",
    "bit_width_bf_sigmoid = 6\n",
    "bit_width_output = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================ Max-min quantization value ============================================\n",
    "max_image_q = 2**(bit_width_image-1)-1\n",
    "min_image_q = -2**(bit_width_image-1)\n",
    "max_filter_conv0_q = 2**(bit_width_filter_conv0-1)-1\n",
    "min_filter_conv0_q = -2**(bit_width_filter_conv0-1)\n",
    "max_filter_conv1_q = 2**(bit_width_filter_conv1-1)-1\n",
    "min_filter_conv1_q = -2**(bit_width_filter_conv1-1)\n",
    "max_conv0_q = 2**(bit_width_conv0-1)-1\n",
    "min_conv0_q = -2**(bit_width_conv0-1)\n",
    "max_conv1_q = 2**(bit_width_conv1-1)-1\n",
    "min_conv1_q = -2**(bit_width_conv1-1)\n",
    "max_bias_conv0_q = 2**(bit_width_bias_conv0-1)-1\n",
    "min_bias_conv0_q = -2**(bit_width_bias_conv0-1)\n",
    "max_bias_conv1_q = 2**(bit_width_bias_conv1-1)-1\n",
    "min_bias_conv1_q = -2**(bit_width_bias_conv1-1)\n",
    "max_pooling0_q = 2**(bit_width_pooling0-1)-1\n",
    "min_pooling0_q = -2**(bit_width_pooling0-1)\n",
    "max_pooling1_q = 2**(bit_width_pooling1-1)-1\n",
    "min_pooling1_q = -2**(bit_width_pooling1-1)\n",
    "\n",
    "max_w0_q = 2**(bit_width_w0-1)-1\n",
    "min_w0_q = -2**(bit_width_w0-1)\n",
    "max_w1_q = 2**(bit_width_w1-1)-1\n",
    "min_w1_q = -2**(bit_width_w1-1)\n",
    "max_w2_q = 2**(bit_width_w2-1)-1\n",
    "min_w2_q = -2**(bit_width_w2-1)\n",
    "\n",
    "max_b0_q = 2**(bit_width_b0-1)-1\n",
    "min_b0_q = -2**(bit_width_b0-1)\n",
    "max_b1_q = 2**(bit_width_b1-1)-1\n",
    "min_b1_q = -2**(bit_width_b1-1)\n",
    "max_b2_q = 2**(bit_width_b2-1)-1\n",
    "min_b2_q = -2**(bit_width_b2-1)\n",
    "\n",
    "max_input_q = 2**(bit_width_input-1)-1\n",
    "min_input_q = -2**(bit_width_input-1)\n",
    "max_hd0_q = 2**(bit_width_h0-1)-1\n",
    "min_hd0_q = -2**(bit_width_h0-1)\n",
    "max_hd1_q = 2**(bit_width_h1-1)-1\n",
    "min_hd1_q = -2**(bit_width_h1-1)\n",
    "max_bf_sigmoid_q = 2**(bit_width_bf_sigmoid-1)-1\n",
    "min_bf_sigmoid_q = -2**(bit_width_bf_sigmoid-1)\n",
    "max_output_q = 2**(bit_width_output-1)-1\n",
    "min_output_q = -2**(bit_width_output-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================ Scale and offset for w1 & bias ============================================\n",
    "# Quantied number = (X/Scale) + Offset\n",
    "scale_image = (max_image - min_image)/(max_image_q - min_image_q)\n",
    "scale_filter_conv0 = (max_filter_conv0 - min_filter_conv0)/(max_filter_conv0_q - min_filter_conv0_q)\n",
    "scale_filter_conv1 = (max_filter_conv1 - min_filter_conv1)/(max_filter_conv1_q - min_filter_conv1_q)\n",
    "scale_conv0 = (max_result_conv0 - min_result_conv0)/(max_conv0_q - min_conv0_q)\n",
    "scale_conv1 = (max_result_conv1 - min_result_conv1)/(max_conv1_q - min_conv1_q)\n",
    "scale_bias_conv0 = (max_bias_conv0 - min_bias_conv0)/(max_bias_conv0_q - min_bias_conv0_q)\n",
    "scale_bias_conv1 = (max_bias_conv1 - min_bias_conv1)/(max_bias_conv1_q - min_bias_conv1_q)\n",
    "scale_pooling0 = (max_pooling0 - min_pooling0)/(max_pooling0_q - min_pooling0_q)\n",
    "scale_pooling1 = (max_pooling1 - min_pooling1)/(max_pooling1_q - min_pooling1_q)\n",
    "\n",
    "scale_w0 = (max_w0 - min_w0)/(max_w0_q - min_w0_q)\n",
    "scale_w1 = (max_w1 - min_w1)/(max_w1_q - min_w1_q)\n",
    "scale_w2 = (max_w2 - min_w2)/(max_w2_q - min_w2_q)\n",
    "\n",
    "scale_b0 = (max_b0 - min_b0)/(max_b0_q - min_b0_q)\n",
    "scale_b1 = (max_b1 - min_b1)/(max_b1_q - min_b1_q)\n",
    "scale_b2 = (max_b2 - min_b2)/(max_b2_q - min_b2_q)\n",
    "\n",
    "scale_input = (max_in - min_in)/(max_input_q - min_input_q)\n",
    "scale_hd0 = (max_hd0 - min_hd0)/(max_hd0_q - min_hd0_q)\n",
    "scale_hd1 = (max_hd1 - min_hd1)/(max_hd1_q - min_hd1_q)\n",
    "scale_bf_sigmoid = (max_out_bf_act - min_out_bf_act)/(max_bf_sigmoid_q - min_bf_sigmoid_q)\n",
    "scale_output = (max_out - min_out)/(max_output_q - min_output_q)\n",
    "\n",
    "offset_image = round((max_image*min_image_q - min_image*max_image_q)/(max_image - min_image))\n",
    "offset_filter_conv0 = round((max_filter_conv0*min_filter_conv0_q - min_filter_conv0*max_filter_conv0_q)/(max_filter_conv0 - min_filter_conv0))\n",
    "offset_filter_conv1 = round((max_filter_conv1*min_filter_conv1_q - min_filter_conv1*max_filter_conv1_q)/(max_filter_conv1 - min_filter_conv1))\n",
    "offset_conv0 = round((max_result_conv0*min_conv0_q - min_result_conv0*max_conv0_q)/(max_result_conv0 - min_result_conv0))\n",
    "offset_conv1 = round((max_result_conv1*min_conv1_q - min_result_conv1*max_conv1_q)/(max_result_conv1 - min_result_conv1))\n",
    "offset_bias_conv0 = round((max_bias_conv0*min_bias_conv0_q - min_bias_conv0*max_bias_conv0_q)/(max_bias_conv0 - min_bias_conv0))\n",
    "offset_bias_conv1 = round((max_bias_conv1*min_bias_conv1_q - min_bias_conv1*max_bias_conv1_q)/(max_bias_conv1 - min_bias_conv1))\n",
    "offset_pooling0 = round((max_pooling0*min_pooling0_q - min_pooling0*max_pooling0_q)/(max_pooling0 - min_pooling0))\n",
    "offset_pooling1 = round((max_pooling1*min_pooling1_q - min_pooling1*max_pooling1_q)/(max_pooling1 - min_pooling1))\n",
    "\n",
    "offset_w0 = round((max_w0*min_w0_q - min_w0*max_w0_q)/(max_w0 - min_w0))\n",
    "offset_w1 = round((max_w1*min_w1_q - min_w1*max_w1_q)/(max_w1 - min_w1))\n",
    "offset_w2 = round((max_w2*min_w2_q - min_w2*max_w2_q)/(max_w2 - min_w2))\n",
    "\n",
    "offset_b0 = round((max_b0*min_b0_q - min_b0*max_b0_q)/(max_b0 - min_b0))\n",
    "offset_b1 = round((max_b1*min_b1_q - min_b1*max_b1_q)/(max_b1 - min_b1))\n",
    "offset_b2 = round((max_b2*min_b2_q - min_b2*max_b2_q)/(max_b2 - min_b2))\n",
    "\n",
    "offset_input = round((max_in*min_input_q - min_in*max_input_q)/(max_in - min_in))\n",
    "offset_hd0 = round((max_hd0*min_hd0_q - min_hd0*max_hd0_q)/(max_hd0 - min_hd0))\n",
    "offset_hd1 = round((max_hd1*min_hd1_q - min_hd1*max_hd1_q)/(max_hd1 - min_hd1))\n",
    "offset_bf_sigmoid = round((max_out_bf_act*min_bf_sigmoid_q - min_out_bf_act*max_bf_sigmoid_q)/(max_out_bf_act - min_out_bf_act))\n",
    "offset_output = round((max_out*min_output_q - min_out*max_output_q)/(max_out - min_out))\n",
    "\n",
    "# ============================================ Scale for quantize ============================================\n",
    "# X * Scale \n",
    "scale_calculate_convolution_0 = (scale_image*scale_filter_conv0)/scale_conv0\n",
    "scale_for_pooling0 = scale_conv0/scale_pooling0\n",
    "scale_calculate_convolution_1 = (scale_pooling0*scale_filter_conv1)/scale_conv1\n",
    "scale_for_pooling1 = scale_conv1/scale_pooling1\n",
    "\n",
    "scale_for_W0_x_Flatten = (scale_input*scale_w0)/scale_hd0\n",
    "scale_for_W1_x_hd0 = (scale_hd0*scale_w1)/scale_hd1\n",
    "scale_for_W2_x_hd1 = (scale_hd1*scale_w2)/scale_bf_sigmoid\n",
    "\n",
    "scale_for_B0_conv = scale_bias_conv0/scale_conv0\n",
    "scale_for_B1_conv = scale_bias_conv1/scale_conv1\n",
    "\n",
    "scale_for_B0_fc = scale_b0/scale_hd0\n",
    "scale_for_B1_fc = scale_b1/scale_hd1\n",
    "scale_for_B2_fc = scale_b2/scale_bf_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "scale_image 0.031746031746031744\nscale_filter_conv0 0.020317460317460317\nscale_filter_conv1 0.02984126984126984\nscale_conv0 0.08476190476190476\nscale_conv1 0.28190476190476194\nscale_bias_conv0 0.01111111111111111\nscale_bias_conv1 0.005079365079365079\nscale_pooling0 0.05555555555555555\nscale_pooling1 0.09015873015873016\nscale_w0 0.03238095238095238\nscale_w1 0.024761904761904763\nscale_w2 0.026984126984126982\nscale_b0 0.006666666666666666\nscale_b1 0.006031746031746032\nscale_b2 0.007619047619047619\nscale_input 0.09015873015873016\nscale_hd0 0.30476190476190473\nscale_hd1 0.40634920634920635\nscale_bf_softmax 0.7428571428571428\nscale_output 0.031746031746031744\nscale_calculate_convolution_0 0.007609535699423339\nscale_for_pooling0 1.5257142857142858\nscale_calculate_convolution_1 0.005880880880880879\nscale_for_pooling1 3.126760563380282\nscale_for_W0_x_Flatten 0.00957936507936508\nscale_for_W1_x_hd0 0.018571428571428572\nscale_for_W2_x_hd1 0.014760548093881429\nscale_for_B0_conv 0.13108614232209737\nscale_for_B1_conv 0.018018018018018014\nscale_for_B0_fc 0.021875000000000002\nscale_for_B1_fc 0.014843750000000001\nscale_for_B2_fc 0.010256410256410258\n"
     ]
    }
   ],
   "source": [
    "print('scale_image', scale_image)\n",
    "print('scale_filter_conv0', scale_filter_conv0)\n",
    "print('scale_filter_conv1', scale_filter_conv1)\n",
    "print('scale_conv0', scale_conv0)\n",
    "print('scale_conv1', scale_conv1)\n",
    "print('scale_bias_conv0', scale_bias_conv0)\n",
    "print('scale_bias_conv1', scale_bias_conv1)\n",
    "print('scale_pooling0', scale_pooling0)\n",
    "print('scale_pooling1', scale_pooling1)\n",
    "print('scale_w0', scale_w0)\n",
    "print('scale_w1', scale_w1)\n",
    "print('scale_w2', scale_w2)\n",
    "print('scale_b0', scale_b0)\n",
    "print('scale_b1', scale_b1)\n",
    "print('scale_b2', scale_b2)\n",
    "print('scale_input', scale_input)\n",
    "print('scale_hd0', scale_hd0)\n",
    "print('scale_hd1', scale_hd1)\n",
    "print('scale_bf_sigmoid', scale_bf_sigmoid)\n",
    "print('scale_output', scale_output)\n",
    "print('scale_calculate_convolution_0', scale_calculate_convolution_0)\n",
    "print('scale_for_pooling0', scale_for_pooling0)\n",
    "print('scale_calculate_convolution_1', scale_calculate_convolution_1)\n",
    "print('scale_for_pooling1', scale_for_pooling1)\n",
    "print('scale_for_W0_x_Flatten', scale_for_W0_x_Flatten)\n",
    "print('scale_for_W1_x_hd0', scale_for_W1_x_hd0)\n",
    "print('scale_for_W2_x_hd1', scale_for_W2_x_hd1)\n",
    "print('scale_for_B0_conv', scale_for_B0_conv)\n",
    "print('scale_for_B1_conv', scale_for_B1_conv)\n",
    "print('scale_for_B0_fc', scale_for_B0_fc)\n",
    "print('scale_for_B1_fc', scale_for_B1_fc)\n",
    "print('scale_for_B2_fc', scale_for_B2_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv0 = Quantization4d(Conv0, min_filter_conv0_q, max_filter_conv0_q, scale_filter_conv0, 0)\n",
    "Bias_conv0 = Quantization2d(Bias_conv0, min_bias_conv0_q, max_bias_conv0_q, scale_bias_conv0, 0)\n",
    "Conv1 = Quantization4d(Conv1, min_filter_conv1_q, max_filter_conv1_q, scale_filter_conv1, 0)\n",
    "Bias_conv1 = Quantization2d(Bias_conv1, min_bias_conv1_q, max_bias_conv1_q, scale_bias_conv1, 0)\n",
    "\n",
    "W0 = Quantization2d(W0, min_w0_q, max_w0_q, scale_w0, 0)\n",
    "W1 = Quantization2d(W1, min_w1_q, max_w1_q, scale_w1, 0)\n",
    "W2 = Quantization2d(W2, min_w2_q, max_w2_q, scale_w2, 0)\n",
    "\n",
    "B0 = Quantization2d(B0, min_b0_q, max_b0_q, scale_b0, 0)\n",
    "B1 = Quantization2d(B1, min_b1_q, max_b1_q, scale_b1, 0)\n",
    "B2 = Quantization2d(B2, min_b2_q, max_b2_q, scale_b2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing...  0 th test\n",
      "Processing...  100 th test\n",
      "Processing...  200 th test\n",
      "Processing...  300 th test\n",
      "Processing...  400 th test\n",
      "Processing...  500 th test\n",
      "Processing...  600 th test\n",
      "Processing...  700 th test\n",
      "Processing...  800 th test\n",
      "Processing...  900 th test\n",
      "Processing...  1000 th test\n",
      "Processing...  1100 th test\n",
      "Processing...  1200 th test\n",
      "Processing...  1300 th test\n",
      "Processing...  1400 th test\n",
      "Processing...  1500 th test\n",
      "Processing...  1600 th test\n",
      "Processing...  1700 th test\n",
      "Processing...  1800 th test\n",
      "Processing...  1900 th test\n",
      "Processing...  2000 th test\n",
      "Processing...  2100 th test\n",
      "Processing...  2200 th test\n",
      "Processing...  2300 th test\n",
      "Processing...  2400 th test\n",
      "Processing...  2500 th test\n",
      "Processing...  2600 th test\n",
      "Processing...  2700 th test\n",
      "Processing...  2800 th test\n",
      "Processing...  2900 th test\n",
      "Processing...  3000 th test\n",
      "Processing...  3100 th test\n",
      "Processing...  3200 th test\n",
      "Processing...  3300 th test\n",
      "Processing...  3400 th test\n",
      "Processing...  3500 th test\n",
      "Processing...  3600 th test\n",
      "Processing...  3700 th test\n",
      "Processing...  3800 th test\n",
      "Processing...  3900 th test\n",
      "Processing...  4000 th test\n",
      "Processing...  4100 th test\n",
      "Processing...  4200 th test\n",
      "Processing...  4300 th test\n",
      "Processing...  4400 th test\n",
      "Processing...  4500 th test\n",
      "Processing...  4600 th test\n",
      "Processing...  4700 th test\n",
      "Processing...  4800 th test\n",
      "Processing...  4900 th test\n",
      "Processing...  5000 th test\n",
      "Processing...  5100 th test\n",
      "Processing...  5200 th test\n",
      "Processing...  5300 th test\n",
      "Processing...  5400 th test\n",
      "Processing...  5500 th test\n",
      "Processing...  5600 th test\n",
      "Processing...  5700 th test\n",
      "Processing...  5800 th test\n",
      "Processing...  5900 th test\n",
      "Processing...  6000 th test\n",
      "Processing...  6100 th test\n",
      "Processing...  6200 th test\n",
      "Processing...  6300 th test\n",
      "Processing...  6400 th test\n",
      "Processing...  6500 th test\n",
      "Processing...  6600 th test\n",
      "Processing...  6700 th test\n",
      "Processing...  6800 th test\n",
      "Processing...  6900 th test\n",
      "Processing...  7000 th test\n",
      "Processing...  7100 th test\n",
      "Processing...  7200 th test\n",
      "Processing...  7300 th test\n",
      "Processing...  7400 th test\n",
      "Processing...  7500 th test\n",
      "Processing...  7600 th test\n",
      "Processing...  7700 th test\n",
      "Processing...  7800 th test\n",
      "Processing...  7900 th test\n",
      "Processing...  8000 th test\n",
      "Processing...  8100 th test\n",
      "Processing...  8200 th test\n",
      "Processing...  8300 th test\n",
      "Processing...  8400 th test\n",
      "Processing...  8500 th test\n",
      "Processing...  8600 th test\n",
      "Processing...  8700 th test\n",
      "Processing...  8800 th test\n",
      "Processing...  8900 th test\n",
      "Processing...  9000 th test\n",
      "Processing...  9100 th test\n",
      "Processing...  9200 th test\n",
      "Processing...  9300 th test\n",
      "Processing...  9400 th test\n",
      "Processing...  9500 th test\n",
      "Processing...  9600 th test\n",
      "Processing...  9700 th test\n",
      "Processing...  9800 th test\n",
      "Processing...  9900 th test\n",
      "Time to handle:  3298.1215925216675 s\n",
      "0.9316\n"
     ]
    }
   ],
   "source": [
    "num_test = 10000\n",
    "num_right = 0\n",
    "\n",
    "pre_time = time.time()\n",
    "\n",
    "for i in range(num_test):\n",
    "    if(i % 100 == 0):\n",
    "        print('Processing... ', i, 'th test')\n",
    "\n",
    "    quatized_image = Quantization3d(image_test[i], min_input_q, max_input_q, scale_input, 0)\n",
    "\n",
    "    result = Feed_forward_Lenet_5_quantization(quatized_image, Conv0, Bias_conv0, Conv1, Bias_conv1, W0, W1, W2, B0, B1, B2, scale_calculate_convolution_0, scale_for_pooling0, scale_calculate_convolution_1, scale_for_pooling1, scale_for_W0_x_Flatten, scale_for_W1_x_hd0, scale_for_W2_x_hd1, scale_output, scale_for_B0_conv, scale_for_B1_conv, scale_for_B0_fc, scale_for_B1_fc, scale_for_B2_fc, max_conv0_q, min_conv0_q, max_conv1_q, min_conv1_q, max_pooling0_q, min_pooling0_q, max_pooling1_q, min_pooling1_q, max_hd0_q, min_hd0_q, max_hd1_q, min_hd1_q, max_bf_sigmoid_q, min_bf_sigmoid_q, scale_bf_sigmoid)\n",
    "\n",
    "    if(result == label_test[i]):\n",
    "        num_right = num_right + 1\n",
    "\n",
    "delta_time = (time.time() - pre_time)\n",
    "print('Time to handle: ', delta_time, 's')\n",
    "\n",
    "accuracy = num_right / num_test\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}